\section{Cost Model}

In this section we will talk about the cost models of the farm and data parallel paradigms used to parallelize the algorithm above. In both of them there is replication of the function in every worker, but in the second we have partition of data. The methodology used for measure the performance is based on standard measures, in particular we have taken principally into account the service time and the scalability. These cost models are based on another cost model regarding the communications between processes: the latency of a send is composed by a fixed term $T_{setup}$ independent from the message length, and by a term $T_{trasm}$ variable in the length of the message.

When we instantiated the problem, we assumed that the emitter in the farm and the scattering module in the data parallel have always an image ready to send. In this way we simulated the minimal interarrival time of images from the source generating the stream (that it will be the camera module during the real behaviour of this application).

\subsection{Farm}

Let's analyze the service time $T_{s}$. 
In the farm, the emitter, the generic worker and the collector are organized in pipeline, hence it is equal to
\[
T_{s} = \max \lbrace T_{e}, T_{w}, T_{c}\rbrace
\]
where, in this specific case, the service time of the emitter (called $T_{e}$) is equal to the communication latency $L_{image}$ for sending an image.
The collector service time $T_{c}$ is negligible respect $T_{e}$ because this module sends only the result of the computation which have a really small data size.
Following the considerations above the term $L_{image}$ is equal to minimum interarrival time $T_{a}$ at the system. 
According to queuing theory we have that the service time $T_{s}$ of the farm system is equal to the interarrival time $L_{image}$ if
\[
T_{w} \leq L_{image} \Leftrightarrow \frac{T_{image}}{nw} \leq L_{image} \Leftrightarrow nw = \big \lceil \frac{T_{image}}{L_{image}} \big \rceil
\]
where $T_{image}$ is the time spent to execute the Gauss-Newton algorithm over an image and $nw$ is the ideal number of workers. If the purpose is to maximize the bandwidth (loosing a few in efficiency) we can choose the ceiling of this value.

If we let $m$ as the stream length, the completion time can be evaluated as
\[
T_{C} = nw \cdot L_{image} + \frac{m}{nw} \cdot T_{w} + T_{c} 
\]
where $\frac{m}{nw}$ is the mean number of tasks for worker.
If we have that $m \gg nw+2$ ( stream length is sufficiently greater than the number of modules of the farm) we can consider the completion time close to
\begin{equation}
\label{completiontime}
T_{C} \simeq m \cdot T_{s}
\end{equation}
The scalability is a measure of how much faster is the computation with respect than the sequential case. 
Formally, it is defined as
\begin{equation}
\label{scalability}
s = \frac{T_{image}}{T_{s}}
\end{equation}
where $T_{s}$ changes in function of the parallelism degree $n=nw+2$.

The Gauss-Newton algorithm practically executes a function for each pixel in the image. 
The time spent for this function is fixed and independent of the pixel value hence we have perfect loading balance and a round-robin strategy is a good solution. 
In case of tasks with different completion time it is important to find a scheduling solution for balancing the load among workers; an on-demand strategy for instance. 
As we will see in the next sections, the tests will confirm that there is no difference about the circular assignment of images and the on-demand strategy in our case.

\subsection{Map Reduce}

The data parallel paradigm can be able to reduce the latency of the computation, but in this case it operates on a stream of elements hence the most important parameter is yet the service time. 
As we said, the data parallel computation used for this parallel program is a map with reduce.
In this case each worker computes the algorithm on his own partition of data thus there is not need to have communications between workers but it is necessary to have scatter and reduce communications. 
The scheme of these communications is very important: in the simplest case we have $nw$ send from or to the workers thus linear latency with respect to the number of nodes.
Then it is possible to map communications on a tree schemes in such a way that the latency becomes logarithmic in the number of workers. 

In the most general case for a map reduce operating on a stream of elements, the service time can be evaluated as
\[
T_{s} = \max\lbrace T_{scatter}, T_{w}, T_{reduce}\rbrace
\] 
Obviously, $T_{w}$ is the time spent to execute our algorithm over a partition of the image. 
In order to obtain maximum bandwidth we choose $nw$ workers such that
\[
nw = \frac{T_{image}}{L_{image}}
\]
and considering scatter and reduce realized in an opportune scheme such that
\[
T_{scatter} = nw*T_{setup} + M*T_{trasm} \leq L_{image}
\]
where $nw$ is the number of workers and $M$ the dimension of the scattered matrix,
\[
T_{reduce} \leq L_{image}
\] 
In this specific case the associative function of the reduce is simply a sum over the partial results obtained by the execution of the algorithm over the single partitions of the image. 
This means, as we said in Section 1.3, that the time spent for this operation is negligible so further considerations about the linear or logarithmic latency are principally referred to the scatter operation only.

The completion time is evaluated like in Formula \ref{completiontime} and the scalability is the same of Formula \ref{scalability}.

\subsection{Farm vs Map}

In general the data parallel solutions require less memory with respect to the farm scheme since the data is partitioned among the workers. 
This can become important in shared memory architecture especially when we increase the grain of the computation.
Latency is also reduced even if this is not fundamental in our case.
On the other hand communication patterns on a map reduce scheme are more complex and at a finer grain. 
This could have a negative impact depending on the architecture.
This latter aspect is fundamental too: to have an optimal dimension of the input data permits to increase the benefits because, in according to our communications cost model above, we can amortize the $T_{setup}$ cost.
In the data parallel form this means to found the dimension of the partitions (hence the parallelism degree) of the matrix, instead in the farm this means to have an adequate dimension of the input images.
We implemented and tested our application exploiting both paradigms with different data grain in such a way to recognize the above pattern.

We know that in both cases the number of workers $nw$ is the same, hence the parallelism degree of our application becomes
\begin{itemize}
\item $nw+2$ in the farm
\item $nw+n_{scatter} + 1$ in the map reduce
\end{itemize}
If the linear scatter is not a bottleneck we have the same parallelism degree of the farm, otherwise some nodes of our architecture will be allocated as service processes.
One process is also fixed as collector of the reduce operation.
We will explain in the next section how we actually implemented the different solutions.