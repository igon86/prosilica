\section{Cost Model}

In this section we will talk about the cost models of the farm and data parallel paradigms used to parallelize the alghorithm above. In both of them there is replication of the function in every worker, but in the second we have partition of data. The methodology used for measure the performance is based on standard measures, in particular we have taken principally into account the service time and the scalability. When we instantiated the problem, we assumed that the emitter in the farm and the scattering module in the data parallel have always an image ready to send. In this way we simulated the minimal interarrival time of images from another module generating the stream (that it will be the camera module during the real behaviour of this application).

\subsection{Farm}

The most important parameter of this paradigm, that is working with computations based on stream, is the service time $T_{s}$. In the farm, the emitter, the generic worker and the collector are organized in pipeline, hence it is equal to
\[
T_{s} = \max \lbrace T_{e}, T_{w}, T_{c}\rbrace
\]
where, in this specific case, the service time of the emitter (called $T_{e}$) is equal to the communication latency $L_{image}$ for sending an image and the collector service time $T_{c}$ is negligible respect $T_{e}$ because this module sends only the result of the computation. Following the considerations above the term $L_{image}$ is equal to minimum interarrival time $T_{a}$ at the system. According to queuing theory we have that the service time $T_{s}$ of the farm system is equal to the interarrival time $L_{image}$ if
\[
T_{w} = L_{image} \Leftrightarrow \frac{T_{image}}{nw} = L_{image} \Leftrightarrow nw = \frac{T_{image}}{L_{image}} 
\]
where $T_{image}$ is the time spent to execute the Gauss-Newton algorithm over an image and $nw$ is the ideal number of workers. If the purpose is to maximize the bandwidth (loosing a few in efficiency) we can choose the ceiling of this value.

If we let $m$ as the stream length, the completion time can be evaluated as
\[
T_{C} = nw \cdot L_{image} + \frac{m}{nw} \cdot T_{w} + T_{c} 
\]
where $\frac{m}{nw}$ is the mean number of tasks for worker.
If we have that $m \gg nw+2$ (stream length is more bigger than the number of modules of the farm), like in this project, we can consider the completion time close to
\begin{equation}
\label{completiontime}
T_{C} \simeq m \cdot T_{s}
\end{equation}
The scalability is a measure of how much speed is the computation respect than the sequential case. Formally, it is defined as
\begin{equation}
\label{scalability}
s = \frac{T_{image}}{T_{s}}
\end{equation}
where $T_{s}$ changes in fuction of the parallelism degree $n=nw+2$.

The Gauss-Newton algorithm executes a function for each pixel in the image. The time spent for this function is not variable considering different pixel value, hence we have the maximum loading balance and a round-robin strategy is a well solution. But if this condition was not valid, it was important to find a scheduling solution for balancing the load like, for instance, an on-demand strategy. As we will see in the next sections, the tests will confirm that there is not difference about the circular assignment of images and the on-demand strategy in this project.

\subsection{Map Reduce}

The data parallel paradigm can be able to reduce the latency of the computation, but in this case it operates on a stream of elements hence the most important parameter is yet the service time. The data parallel computation used for this project is a map with reduce. In this case each worker computes the algorithm on own partition of data, hence there is not need to have communications between workers but it is necessary to have scatter and reduce communications. The scheme of these communications is very important: in the simplest case we have $nw$ send from or to the workers but is possible to think about tree schemes where the latency becomes logarithmic in the number of workers and no more linear. Another aspect to take into account is the how to partition the pixels of the image: it can be done by rows, columns or in an interleaving way. In this section we are not interesting in details regarding the implementation but our cost model will take in consideration these choices.

In the most general case for a map reduce operating on a stream of elements, the service time can be evaluated as
\[
T_{s} = \max\lbrace T_{scatter}, T_{w}, T_{reduce}\rbrace
\]
because, like in the farm, we have a pipeline behaviour. Obviously, $T_{w}$ is the time spent for execute our algorithm over a partition of the image. We obtain that the map is not bottleneck if we choose $nw$ workers such that
\[
nw = \frac{T_{image}}{L_{image}}
\]
and considering scatter and reduce realized in an opportune scheme such that
\[
T_{scatter} \leq L_{image}
\]
\[
T_{reduce} \leq L_{image}
\] 
In this specific case the associative function of the reduce is simply a sum over the partial results obtained by the execution over a single partition of the image. This means that this term is negligible and there is not need to realize it using a tree but it is sufficient to use only a module. In this project, the considerations above are applied principally to the scatter. In particular we decided to partition the pixels of the image by rows obtainign a minimum number of fault regarding the cache.

The completion time is evaluated like in Formula \ref{completiontime} and the scalability is the same of Formula \ref{scalability}.

\subsection{Farm vs Map}
